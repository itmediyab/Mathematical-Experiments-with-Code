[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Studio 2: Mathematical Experiments with Code",
    "section": "",
    "text": "Name: Diya Bijoy\nAdmission ID: 235306018\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAssignment 2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAssignment 1\n\n\n\n\n\n\n\n\n\n\n\nMar 31, 2025\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Assignment 1",
    "section": "",
    "text": "Rayna Singh, Diya Bijoy, Aanya Pandith, Naman Rajoria, Soumya Saboo\n\n\n\nImage of Generative Garden\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLink to P5.js code"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/post-with-code/index.html#a1-generative-garden",
    "href": "posts/post-with-code/index.html#a1-generative-garden",
    "title": "Assignment 1",
    "section": "",
    "text": "Rayna Singh, Diya Bijoy, Aanya Pandith, Naman Rajoria, Soumya Saboo\n\n\n\nImage of Generative Garden\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLink to P5.js code"
  },
  {
    "objectID": "posts/post-with-code/index.html#concept-note",
    "href": "posts/post-with-code/index.html#concept-note",
    "title": "Assignment 1",
    "section": "Concept Note",
    "text": "Concept Note\n\nIn Islamic culture, creating a garden is equivalent to building a piece of heaven on earth. The main objective of this art piece is to fabricate an immersive 3D garden that reimagines natural landscapes through digital media using code. The title “Anavrin,” is Nirvana spelled backwards, the reason for initiating this title is to reinstate the serenity that a garden brings to the visitor. Our main objective is to mirror a real-world garden, or rather our interpretation of the same, onto a digital stage. Hence, we use the mirror as our main conduit for this exercise and hold up a mirror on the word ‘Nirvana.’ Nirvana can be interpreted as a place or state of oblivion to care, pain, or external reality.\nOur intent wasn’t just to create a mere replication of a garden using the varied and vibrant mathematical functions that we learnt. Rather, we wanted to create a body of work that echoes it’s origin, that is laid with embedded metaphors and meaning. We wanted it to be both a visual and philosophical exploration and creation.\nThis project blends art, ecology, and technology to create an environment that evolves based on the contributions of each teammate. We have established pathways for the viewers or spectators to not only eye the art piece but interact with it. Our philosophy dictates that if art can’t make you stop, look and interact, if it can’t make you think, there is no point in calling it art. We didn’t want to infer the metaphor or the alt text we use in this, instead, we left it to the viewer with any discretion to figure out how the garden is laid out and the purpose of each element. Parallel to the beginning of every epic, we start off with the sky. The sky is the poetry cloud, a cluster of disconinuities laid out in a channel, they indicate the presence of divinity and pivot change. Then we proceed to the flowers, that change colour and shape and position, they embody all material objects. After which we have the trees, which grow and perish, they are used to metaphorise change. We have butterflies in the composition that embody the untimely notion of life and death and life after death. Finally we have a pond, a water body to symbolise rejuvenation.\nUsing p5.js, WEBL and learning how to put everything together felt difficult and overwhelming to a lot of us. But we persisted and figured out how to navigate through the tough times. For the purpose of creating our digital ecosystem, we considered and looked to Mother Nature for inspiration. We developed and formulated an ecosystem. Each element is a contribution of each individual and their creative expression in the most expressive way. It also reflects the diversity and convergence of our minds and ideas and how they all formulate and meet at a common ground. The purpose of this art piece is to evoke the sense of being in a garden and trying to visualise the garden but in a more visceral manner.\n\n\nGarden Elements & Math Concepts\n1) Flowers\nMath Concept: Parametric Equations\n\nThe flower shape is defined using parametric equations in polar coordinates.\nIn the code, this equation generates radial symmetry which gives the flowers a structured petal arrangement.\n\n\n\n\n\n\n\n\n2) Terrain\nMath Concept: Julia Fractals\n\nThe Julia set determines which areas are water or land.\nFormula: z=z^2+c\nThe rule takes a number, changes it using a formula and checks if it stays small or grows big.\nIn the code, it creates a procedural landscape where blue areas represent water and green areas represent land.\n\n\n\n\n\n3) Trees\nMath Concept: Geometric Progression\n\nGeometric progression is a sequence where each term is multiplied by a fixed ratio.\nLn​=L0​×rn, where the branch length shrinks by 70% each step.\nEach branch recursively creates three smaller branches, reducing in size until the length is 10 or less.\n\n\n\n\n\n4) Butterflies\nMath Concept: Circular Motion\n\nCircular motion describes movement along a curved path where an object’s position changes based on sine and cosine functions.\nFormula: x=rcos(θ),y=rsin(θ)\nEach butterfly moves in a circular path by updating its xxx and yyy coordinates using cosine and sine functions.\n\n\n\n\n\n6) Clouds\nMath Concept: Euclidean Distance\n\nEuclidean distance measures how far two points are from each other in a straight line.\nFormula:\nd=(x2−x1)2+(y2−y1)2d = d=(x2​−x1​)2+(y2​−y1​)2​\nIn the code, it calculates the distance between the mouse and each cloud to adjust their size and transparency.\nCloser clouds appear bigger and brighter, while farther clouds appear smaller and more transparent.\n\n\n\n\n\n\n\n\nProcess & Practise\nWe began by exploring natural patterns and generative algorithms, experimenting with different plant growth models, fractals and interactive elements."
  },
  {
    "objectID": "posts/A2/Assignment2.html",
    "href": "posts/A2/Assignment2.html",
    "title": "Assignment 2",
    "section": "",
    "text": "Sara Saju, Diya Bijoy, Bunyan Usman, Ayush Garg, Sheil Srimany"
  },
  {
    "objectID": "posts/A2/Assignment2.html#a2",
    "href": "posts/A2/Assignment2.html#a2",
    "title": "Assignment 2",
    "section": "",
    "text": "Sara Saju, Diya Bijoy, Bunyan Usman, Ayush Garg, Sheil Srimany"
  },
  {
    "objectID": "posts/a2/CopyOfindex.html",
    "href": "posts/a2/CopyOfindex.html",
    "title": "Assignment 1",
    "section": "",
    "text": "Sara Saju, Diya Bijoy, Bunyan Usman, Ayush Garg and Sheil Srimany"
  },
  {
    "objectID": "posts/a2/CopyOfindex.html#a1-dance-mudra-detection",
    "href": "posts/a2/CopyOfindex.html#a1-dance-mudra-detection",
    "title": "Assignment 1",
    "section": "",
    "text": "Sara Saju, Diya Bijoy, Bunyan Usman, Ayush Garg and Sheil Srimany"
  },
  {
    "objectID": "posts/a2/CopyOfindex.html#process",
    "href": "posts/a2/CopyOfindex.html#process",
    "title": "Assignment 1",
    "section": "Process",
    "text": "Process"
  },
  {
    "objectID": "posts/a2/CopyOfindex.html#a2-dance-mudra-detection",
    "href": "posts/a2/CopyOfindex.html#a2-dance-mudra-detection",
    "title": "Assignment 2",
    "section": "",
    "text": "Sara Saju, Diya Bijoy, Bunyan Usman, Ayush Garg and Sheil Srimany"
  },
  {
    "objectID": "posts/a2/CopyOfindex.html#problems-we-ran-into",
    "href": "posts/a2/CopyOfindex.html#problems-we-ran-into",
    "title": "Assignment 2",
    "section": "Problems we ran into",
    "text": "Problems we ran into\n\nWe ran into an issue where the Handpose model detected 43 points instead of 42. To fix this, we removed the “which hand” point from the CSV file, which was causing the extra point."
  },
  {
    "objectID": "posts/A2/A2.html#a2-dance-mudra-detection",
    "href": "posts/A2/A2.html#a2-dance-mudra-detection",
    "title": "Assignment 2",
    "section": "",
    "text": "Sara Saju, Diya Bijoy, Bunyan Usman, Ayush Garg and Sheil Srimany.\n\nThis project uses ml5.js and p5.js to build a real-time mudra recognition system. Based on the recognized mudra, the system plays a corresponding audio track — specifically, Carnatic/Bharatanatyam based classical music for each gesture."
  },
  {
    "objectID": "posts/A2/A2.html#problems-we-ran-into",
    "href": "posts/A2/A2.html#problems-we-ran-into",
    "title": "Assignment 2",
    "section": "Problems we ran into",
    "text": "Problems we ran into\n\nWe had an issue where the Handpose model was detecting 43 points instead of 42. This was because it included an extra “which hand” point. To fix it, we removed that point from the CSV file. After that, the graph showed up properly without any NaN errors.\n\n\n\n\nWe used http://localhost:8000/model/model.json to load our trained model because browsers do not allow direct access to local files for security reasons. By running a local server with python -m http.server 8000, we could load files like model.json and audio properly. This made it possible to test the project as if it were live on a real website. \nWe explored using a Convolutional Neural Network to train our model with TensorFlow, but ran into issues. CNNs requires a large amount of image data and our dataset was too small to train the model effectively. Because of this, we switched to using a Multi-Layer Perceptron.\nWe encountered several issues with the CSV files during the preparation of our dataset. Initially, we tried to extract the 21 keypoints directly by feeding a folder of images into our code. However, this approach led to a lot of missing data—many images failed to return any coordinates.\nWe also attempted a manual method to estimate approximate values, but this didn’t work either.\nThe issue was eventually resolved with help from Aishwarya’s team. They provided us with the initial code to extract the keypoints into a CSV file and suggested that we first generate a separate CSV file (path2.csv) containing the name and path of each image. Using this file, along with the images, we were able to feed the data into the detection model more effectively and extract the 21 hand keypoints. While some images still couldn’t be read—possibly due to background obstructions—the overall number of missing entries was significantly reduced."
  },
  {
    "objectID": "posts/A2/a2.html",
    "href": "posts/A2/a2.html",
    "title": "a2",
    "section": "",
    "text": "Sara Saju, Diya Bijoy, Bunyan Usman, Ayush Garg and Sheil Srimany\n\n\nYour browser does not support the video tag."
  },
  {
    "objectID": "posts/A2/a2.html#a2-dance-mudra-detection",
    "href": "posts/A2/a2.html#a2-dance-mudra-detection",
    "title": "a2",
    "section": "",
    "text": "Sara Saju, Diya Bijoy, Bunyan Usman, Ayush Garg and Sheil Srimany\n\n\nYour browser does not support the video tag."
  },
  {
    "objectID": "posts/A2/a2.html#problems-we-ran-into",
    "href": "posts/A2/a2.html#problems-we-ran-into",
    "title": "a2",
    "section": "Problems we ran into",
    "text": "Problems we ran into\n\nWe ran into an issue where the Handpose model detected 43 points instead of 42. To fix this, we removed the “which hand” point from the CSV file, which was causing the extra point."
  },
  {
    "objectID": "posts/A2/A2.html",
    "href": "posts/A2/A2.html",
    "title": "Assignment 2",
    "section": "",
    "text": "Sara Saju, Diya Bijoy, Bunyan Usman, Ayush Garg and Sheil Srimany.\n\nThis project uses ml5.js and p5.js to build a real-time mudra recognition system. Based on the recognized mudra, the system plays a corresponding audio track — specifically, Carnatic/Bharatanatyam based classical music for each gesture."
  },
  {
    "objectID": "posts/A2/A2.html#main-technologies-used",
    "href": "posts/A2/A2.html#main-technologies-used",
    "title": "A2",
    "section": "Main Technologies Used",
    "text": "Main Technologies Used\n\np5.js (Locally): Used for handling the webcam feed, drawing visuals, sound integration, and processing CSV files locally to extract names, paths of images, and the 21 keypoints data.\nml5.js: For using the handpose model and integrating a trained neural network.\nNeural Network Model: Trained using your own dataset of hand images.\nHTML/CSS: To build the webpage interface and display live predictions.\nVS Code: Used for training both the machine learning (ML) and convolutional neural network (CNN) models.\nPython: Set up the local server for hosting the project and organizing the initial CSV files with image names and paths.\nAI: Refined the code structure for optimal performance, with references from the official ml5.js documentation."
  },
  {
    "objectID": "posts/A2/Assignment 2.html",
    "href": "posts/A2/Assignment 2.html",
    "title": "Assignment 2",
    "section": "",
    "text": "Sara Saju, Diya Bijoy, Bunyan Usman, Ayush Garg and Sheil Srimany  This project uses ml5.js and p5.js to build a real-time mudra recognition system. Based on the recognized mudra, the system plays a corresponding audio track — specifically, Carnatic/Bharatanatyam based classical music for each gesture.\n\n\nYour browser does not support the video tag."
  },
  {
    "objectID": "posts/A2/Assignment 2.html#a2-dance-mudra-detection",
    "href": "posts/A2/Assignment 2.html#a2-dance-mudra-detection",
    "title": "Assignment 2",
    "section": "",
    "text": "Sara Saju, Diya Bijoy, Bunyan Usman, Ayush Garg and Sheil Srimany  This project uses ml5.js and p5.js to build a real-time mudra recognition system. Based on the recognized mudra, the system plays a corresponding audio track — specifically, Carnatic/Bharatanatyam based classical music for each gesture.\n\n\nYour browser does not support the video tag."
  },
  {
    "objectID": "posts/A2/Assignment 2.html#main-technologies-used",
    "href": "posts/A2/Assignment 2.html#main-technologies-used",
    "title": "Assignment 2",
    "section": "Main Technologies Used",
    "text": "Main Technologies Used\n\np5.js (Locally): Used for handling the webcam feed, drawing visuals, sound integration, and processing CSV files locally to extract names, paths of images, and the 21 keypoints data.\nml5.js: For using the handpose model and integrating a trained neural network.\nNeural Network Model: Trained using your own dataset of hand images.\nHTML/CSS: To build the webpage interface and display live predictions.\nVS Code: Used for training both the machine learning (ML) and convolutional neural network (CNN) models.\nPython: Set up the local server for hosting the project and organizing the initial CSV files with image names and paths.\nAI: Refined the code structure for optimal performance, with references from the official ml5.js documentation."
  },
  {
    "objectID": "posts/A2/Assignment 2.html#problems-we-ran-into",
    "href": "posts/A2/Assignment 2.html#problems-we-ran-into",
    "title": "Assignment 2",
    "section": "Problems we ran into",
    "text": "Problems we ran into\n\nWe had an issue where the Handpose model was detecting 43 points instead of 42. This was because it included an extra “which hand” point. To fix it, we removed that point from the CSV file. After that, the graph showed up properly without any NaN errors.\n\n\n \n\n\nWe used http://localhost:8000/model/model.json to load our trained model because browsers do not allow direct access to local files for security reasons. By running a local server with python -m http.server 8000, we could load files like model.json and audio properly. This made it possible to test the project as if it were live on a real website.\nWe explored using a Convolutional Neural Network to train our model with TensorFlow, but ran into issues. CNNs requires a large amount of image data and our dataset was too small to train the model effectively. Because of this, we switched to using a Multi-Layer Perceptron."
  },
  {
    "objectID": "posts/Assignment 2/Assignment 2.html",
    "href": "posts/Assignment 2/Assignment 2.html",
    "title": "Assignment 2",
    "section": "",
    "text": "Sara Saju, Diya Bijoy, Bunyan Usman, Ayush Garg and Sheil Srimany  This project uses ml5.js and p5.js to build a real-time mudra recognition system. Based on the recognized mudra, the system plays a corresponding audio track — specifically, Carnatic/Bharatanatyam based classical music for each gesture."
  },
  {
    "objectID": "posts/Assignment 2/Assignment 2.html#a2-dance-mudra-detection",
    "href": "posts/Assignment 2/Assignment 2.html#a2-dance-mudra-detection",
    "title": "Assignment 2",
    "section": "",
    "text": "Sara Saju, Diya Bijoy, Bunyan Usman, Ayush Garg and Sheil Srimany  This project uses ml5.js and p5.js to build a real-time mudra recognition system. Based on the recognized mudra, the system plays a corresponding audio track — specifically, Carnatic/Bharatanatyam based classical music for each gesture."
  },
  {
    "objectID": "posts/Assignment 2/Assignment 2.html#main-technologies-used",
    "href": "posts/Assignment 2/Assignment 2.html#main-technologies-used",
    "title": "Assignment 2",
    "section": "Main Technologies Used",
    "text": "Main Technologies Used\n\np5.js (Locally): Used for handling the webcam feed, drawing visuals, sound integration, and processing CSV files locally to extract names, paths of images, and the 21 keypoints data.\nml5.js: For using the handpose model and integrating a trained neural network.\nNeural Network Model: Trained using your own dataset of hand images.\nHTML/CSS: To build the webpage interface and display live predictions.\nVS Code: Used for training both the machine learning (ML) and convolutional neural network (CNN) models.\nPython: Set up the local server for hosting the project and organizing the initial CSV files with image names and paths.\nAI: Refined the code structure for optimal performance, with references from the official ml5.js documentation."
  },
  {
    "objectID": "posts/Assignment 2/Assignment 2.html#process",
    "href": "posts/Assignment 2/Assignment 2.html#process",
    "title": "Assignment 2",
    "section": "Process",
    "text": "Process\n\nWe began by taking images of people performing five mudras: Arala, Kapita, Kartari, Kataka Mukha, and Mayura.\nThe images were categorized into separate folders and uploaded to Google Drive.\nWe used Python to generate a file called path2.csv, which contains the name and path of each image.\nThis CSV file, along with the images, was then used to extract the 21 keypoints for each image using the handpose model, resulting in a new file: hand_keypoints.csv.\nWe trained a multi-layer perceptron (MLP) model using this data and received three output files:\n\n\nmodel.json\nmodel_meta.json\nmodel.weights.bin\n\n\nAudio clips for each varnam were sourced from YouTube and added to the project.\nThese files were integrated into p5 for live mudra detection using the webcam and corresponding audio.\nThroughout the process, we ran multiple tests using smaller image sets and CSV files to ensure each part worked before scaling up. Even with this testing, we encountered several challenges when using the full dataset in the final implementation. \n\nLink to live detection\nLink to csv file"
  },
  {
    "objectID": "posts/Assignment 2/Assignment 2.html#problems-we-ran-into",
    "href": "posts/Assignment 2/Assignment 2.html#problems-we-ran-into",
    "title": "Assignment 2",
    "section": "Problems we ran into",
    "text": "Problems we ran into\n\nWe had an issue where the Handpose model was detecting 43 points instead of 42. This was because it included an extra “which hand” point. To fix it, we removed that point from the CSV file. After that, the graph showed up properly without any NaN errors.\n\n\n\n\nWe used http://localhost:8000/model/model.json to load our trained model because browsers do not allow direct access to local files for security reasons. By running a local server with python -m http.server 8000, we could load files like model.json and audio properly. This made it possible to test the project as if it were live on a real website.\nWe explored using a Convolutional Neural Network to train our model with TensorFlow, but ran into issues. CNNs requires a large amount of image data and our dataset was too small to train the model effectively. Because of this, we switched to using a Multi-Layer Perceptron.\nWe encountered several issues with the CSV files during the preparation of our dataset. Initially, we tried to extract the 21 keypoints directly by feeding a folder of images into our code. However, this approach led to a lot of missing data—many images failed to return any coordinates.\nWe also attempted a manual method to estimate approximate values, but this didn’t work either.\nThe issue was eventually resolved with help from Aishwarya’s team. They provided us with the initial code to extract the keypoints into a CSV file and suggested that we first generate a separate CSV file (path2.csv) containing the name and path of each image. Using this file, along with the images, we were able to feed the data into the detection model more effectively and extract the 21 hand keypoints. While some images still couldn’t be read—possibly due to background obstructions—the overall number of missing entries was significantly reduced."
  },
  {
    "objectID": "posts/Assignment 2/Assignment 2.html#main-tools-used",
    "href": "posts/Assignment 2/Assignment 2.html#main-tools-used",
    "title": "Assignment 2",
    "section": "Main Tools Used",
    "text": "Main Tools Used\n\np5.js (Locally): Used for handling the webcam feed, drawing visuals, sound integration, and processing CSV files locally to extract names, paths of images, and the 21 keypoints data.\nml5.js: For using the handpose model and integrating a trained neural network.\nNeural Network Model: Trained using your own dataset of hand images.\nHTML/CSS: To build the webpage interface and display live predictions.\nVS Code: Used for training both the machine learning (ML) and convolutional neural network (CNN) models.\nPython: Set up the local server for hosting the project and organizing the initial CSV files with image names and paths.\nAI: Refined the code structure for optimal performance, with references from the official ml5.js documentation."
  },
  {
    "objectID": "posts/Assignment 2/Assigment 2.html",
    "href": "posts/Assignment 2/Assigment 2.html",
    "title": "Assigment 2",
    "section": "",
    "text": "Sara Saju, Diya Bijoy, Bunyan Usman, Ayush Garg and Sheil Srimany  This project uses ml5.js and p5.js to build a real-time mudra recognition system. Based on the recognized mudra, the system plays a corresponding audio track — specifically, Carnatic/Bharatanatyam based classical music for each gesture."
  },
  {
    "objectID": "posts/Assignment 2/Assigment 2.html#a2-dance-mudra-detection",
    "href": "posts/Assignment 2/Assigment 2.html#a2-dance-mudra-detection",
    "title": "Assigment 2",
    "section": "",
    "text": "Sara Saju, Diya Bijoy, Bunyan Usman, Ayush Garg and Sheil Srimany  This project uses ml5.js and p5.js to build a real-time mudra recognition system. Based on the recognized mudra, the system plays a corresponding audio track — specifically, Carnatic/Bharatanatyam based classical music for each gesture."
  },
  {
    "objectID": "posts/Assignment 2/Assigment 2.html#main-tools-used",
    "href": "posts/Assignment 2/Assigment 2.html#main-tools-used",
    "title": "Assigment 2",
    "section": "Main Tools Used",
    "text": "Main Tools Used\n\np5.js (Locally): Used for handling the webcam feed, drawing visuals, sound integration, and processing CSV files locally to extract names, paths of images, and the 21 keypoints data.\nml5.js: For using the handpose model and integrating a trained neural network.\nNeural Network Model: Trained using your own dataset of hand images.\nHTML/CSS: To build the webpage interface and display live predictions.\nVS Code: Used for training both the machine learning (ML) and convolutional neural network (CNN) models.\nPython: Set up the local server for hosting the project and organizing the initial CSV files with image names and paths.\nAI: Refined the code structure for optimal performance, with references from the official ml5.js documentation."
  },
  {
    "objectID": "posts/Assignment 2/Assigment 2.html#process",
    "href": "posts/Assignment 2/Assigment 2.html#process",
    "title": "Assigment 2",
    "section": "Process",
    "text": "Process\n\nWe began by taking images of people performing five mudras: Arala, Kapita, Kartari, Kataka Mukha, and Mayura.\nThe images were categorized into separate folders and uploaded to Google Drive.\nWe used Python to generate a file called path2.csv, which contains the name and path of each image.\nThis CSV file, along with the images, was then used to extract the 21 keypoints for each image using the handpose model, resulting in a new file: hand_keypoints.csv.\nWe trained a multi-layer perceptron (MLP) model using this data and received three output files:\n\n\nmodel.json\nmodel_meta.json\nmodel.weights.bin\n\n\nAudio clips for each varnam were sourced from YouTube and added to the project.\nThese files were integrated into the web app for live mudra detection using the webcam and corresponding audio.\nThroughout the process, we ran multiple tests using smaller image sets and CSV files to ensure each part worked before scaling up. Even with this testing, we encountered several challenges when using the full dataset in the final implementation.\n\nLink to live detection  Link to csv file"
  },
  {
    "objectID": "posts/Assignment 2/Assigment 2.html#problems-we-ran-into",
    "href": "posts/Assignment 2/Assigment 2.html#problems-we-ran-into",
    "title": "Assigment 2",
    "section": "Problems we ran into",
    "text": "Problems we ran into\n\nWe had an issue where the Handpose model was detecting 43 points instead of 42. This was because it included an extra “which hand” point. To fix it, we removed that point from the CSV file. After that, the graph showed up properly without any NaN errors.\n\n\n \n\n\nWe used http://localhost:8000/model/model.json to load our trained model because browsers do not allow direct access to local files for security reasons. By running a local server with python -m http.server 8000, we could load files like model.json and audio properly. This made it possible to test the project as if it were live on a real website.\nWe explored using a Convolutional Neural Network to train our model with TensorFlow, but ran into issues. CNNs requires a large amount of image data and our dataset was too small to train the model effectively. Because of this, we switched to using a Multi-Layer Perceptron."
  },
  {
    "objectID": "posts/A2/A2.html#main-tools-used",
    "href": "posts/A2/A2.html#main-tools-used",
    "title": "Assignment 2",
    "section": "Main Tools Used",
    "text": "Main Tools Used\n\np5.js (Locally): Used for handling the webcam feed, drawing visuals, sound integration, and processing CSV files locally to extract names, paths of images, and the 21 keypoints data.\nml5.js: For using the handpose model and integrating a trained neural network.\nNeural Network Model: Trained using your own dataset of hand images.\nHTML/CSS: To build the webpage interface and display live predictions.\nVS Code: Used for training both the machine learning (ML) and convolutional neural network (CNN) models.\nPython: Set up the local server for hosting the project and organizing the initial CSV files with image names and paths.\nAI: Refined the code structure for optimal performance, with references from the official ml5.js documentation."
  },
  {
    "objectID": "posts/A2/A2.html#process",
    "href": "posts/A2/A2.html#process",
    "title": "Assignment 2",
    "section": "Process",
    "text": "Process\n\nWe began by taking images of people performing five mudras: Arala, Kapita, Kartari, Kataka Mukha, and Mayura.\nThe images were categorized into separate folders and uploaded to Google Drive.\nWe used Python to generate a file called path2.csv, which contains the name and path of each image.\nThis CSV file, along with the images, was then used to extract the 21 keypoints for each image using the handpose model, resulting in a new file: hand_keypoints.csv.\nWe trained a multi-layer perceptron (MLP) model using this data and received three output files:\n\n\nmodel.json\nmodel_meta.json\nmodel.weights.bin\n\n\nAudio clips for each varnam were sourced from YouTube and added to the project.\nThese files were integrated into p5 for live mudra detection using the webcam and corresponding audio.\nThroughout the process, we ran multiple tests using smaller image sets and CSV files to ensure each part worked before scaling up. Even with this testing, we encountered several challenges when using the full dataset in the final implementation. \n\nLink to live detection\nLink to csv file"
  }
]