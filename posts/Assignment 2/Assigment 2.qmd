---
title: "Assigment 2"
format: html
---

## A2: Dance Mudra Detection

Sara Saju, Diya Bijoy, Bunyan Usman, Ayush Garg and Sheil Srimany <br><br> This project uses **ml5.js** and **p5.js** to build a **real-time mudra recognition system**. Based on the recognized mudra, the system plays a corresponding **audio track** â€” specifically, Carnatic/Bharatanatyam based classical music for each gesture.

<video width="640" height="360" controls>

<source src="finalvideo.mp4" type="video/mp4">

</video>

<br>

## Main Tools Used

-   p5.js (Locally): Used for handling the webcam feed, drawing visuals, sound integration, and processing CSV files locally to extract names, paths of images, and the 21 keypoints data.

-   ml5.js: For using the handpose model and integrating a trained neural network.

-   Neural Network Model: Trained using your own dataset of hand images.

-   HTML/CSS: To build the webpage interface and display live predictions.

-   VS Code: Used for training both the machine learning (ML) and convolutional neural network (CNN) models.

-   Python: Set up the local server for hosting the project and organizing the initial CSV files with image names and paths.

-   AI: Refined the code structure for optimal performance, with references from the official ml5.js documentation.

<br>

## Process

- We began by taking images of people performing five mudras: Arala, Kapita, Kartari, Kataka Mukha, and Mayura.

- The images were categorized into separate folders and uploaded to Google Drive.

- We used Python to generate a file called path2.csv, which contains the name and path of each image.

- This CSV file, along with the images, was then used to extract the 21 keypoints for each image using the handpose model, resulting in a new file: hand_keypoints.csv.

- We trained a multi-layer perceptron (MLP) model using this data and received three output files:
a. model.json
b. model_meta.json
c. model.weights.bin

- Audio clips for each varnam were sourced from YouTube and added to the project.

- These files were integrated into the web app for live mudra detection using the webcam and corresponding audio.

- Throughout the process, we ran multiple tests using smaller image sets and CSV files to ensure each part worked before scaling up. Even with this testing, we encountered several challenges when using the full dataset in the final implementation.

[Link to live detection](https://editor.p5js.org/sarasj0510/full/2FCh_V8x6) <br> [Link to csv file](https://editor.p5js.org/sarasj0510/full/uv0HuV8m-)

## Problems we ran into

-   We had an issue where the Handpose model was detecting 43 points instead of 42. This was because it included an extra "which hand" point. To fix it, we removed that point from the CSV file. After that, the graph showed up properly without any NaN errors.

<div style="display: flex; justify-content: center; gap: 10px; margin-bottom: 20px;">
  <img src="nan_error.jpg" width="400" height="300">
  <img src="without_nan_error.jpg" width="400" height="300">
</div>

-   We used `http://localhost:8000/model/model.json` to load our trained model because browsers do not allow direct access to local files for security reasons. By running a local server with `python -m http.server 8000`, we could load files like `model.json` and audio properly. This made it possible to test the project as if it were live on a real website.
-   We explored using a Convolutional Neural Network to train our model with TensorFlow, but ran into issues. CNNs requires a large amount of image data and our dataset was too small to train the model effectively. Because of this, we switched to using a Multi-Layer Perceptron.
