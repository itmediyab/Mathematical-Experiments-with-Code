---
title: "A2"
format: html
---

## A2: Dance Mudra Detection

Sara Saju, Diya Bijoy, Bunyan Usman, Ayush Garg and Sheil Srimany <br><br> This project uses **ml5.js** and **p5.js** to build a **real-time mudra recognition system**. Based on the recognized mudra, the system plays a corresponding **audio track** â€” specifically, Carnatic/Bharatanatyam based classical music for each gesture.

<video width="640" height="360" controls>

<source src="finalvideo.mp4" type="video/mp4">

Your browser does not support the video tag. </video> 

<iframe src="https://editor.p5js.org/sarasj0510/full/2FCh_V8x6" width="400" height="300">

</iframe>

<br>

## Main Technologies Used

-   p5.js (Locally): Used for handling the webcam feed, drawing visuals, sound integration, and processing CSV files locally to extract names, paths of images, and the 21 keypoints data.

-   ml5.js: For using the handpose model and integrating a trained neural network.

-   Neural Network Model: Trained using your own dataset of hand images.

-   HTML/CSS: To build the webpage interface and display live predictions.

-   VS Code: Used for training both the machine learning (ML) and convolutional neural network (CNN) models.

-   Python: Set up the local server for hosting the project and organizing the initial CSV files with image names and paths.

-   AI: Refined the code structure for optimal performance, with references from the official ml5.js documentation.

<br>

##  Problems we ran into

-   We had an issue where the Handpose model was detecting 43 points instead of 42. This was because it included an extra "which hand" point. To fix it, we removed that point from the CSV file. After that, the graph showed up properly without any NaN errors.

<div style="display: flex; justify-content: center; gap: 10px; margin-bottom: 20px;">
  <img src="nan_error.jpg" width="400" height="300" alt="First image description">
  <img src="without_nan_error.jpg" width="400" height="300" alt="Second image description">
</div>

-   We used `http://localhost:8000/model/model.json` to load our trained model because browsers do not allow direct access to local files for security reasons. By running a local server with `python -m http.server 8000`, we could load files like `model.json` and audio properly. This made it possible to test the project as if it were live on a real website.
-   We explored using a Convolutional Neural Network to train our model with TensorFlow, but ran into issues. CNNs requires a large amount of image data and our dataset was too small to train the model effectively. Because of this, we switched to using a Multi-Layer Perceptron.